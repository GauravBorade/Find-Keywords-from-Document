{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Keywords from the document\n",
    "In this notebook, we are going to extract the keywords from the document shared in the link.\n",
    "\n",
    "Original Document link is provided below.\n",
    "\n",
    "Link: http://bit.ly/epo_keyword_extraction_document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary packages\n",
    "# For basic string,text operation import following\n",
    "import re, string, unicodedata\n",
    "\n",
    "# Natural Language toolkit (nltk) used for text processing\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "\n",
    "#Importing necessary package for pdf to word conversion\n",
    "\n",
    "import os\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "# From PDFInterpreter import both PDFResourceManager and PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "# Import this to raise exception whenever text extraction from PDF is not allowed\n",
    "from pdfminer.pdfpage import PDFTextExtractionNotAllowed\n",
    "from pdfminer.layout import LAParams, LTTextBox, LTTextLine\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/Dipti_B/Desktop/ds_keyword_assignment/Oreilly.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-160f8fe06b4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Open and read the pdf file in binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# Create parser object to parse the pdf content\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/Dipti_B/Desktop/ds_keyword_assignment/Oreilly.pdf'"
     ]
    }
   ],
   "source": [
    "''' This is what we are trying to do:\n",
    "1) Transfer information from PDF file to PDF document object. This is done using parser\n",
    "2) Open the PDF file\n",
    "3) Parse the file using PDFParser object\n",
    "4) Assign the parsed content to PDFDocument object\n",
    "5) Now the information in this PDFDocumet object has to be processed. For this we need\n",
    "   PDFPageInterpreter, PDFDevice and PDFResourceManager\n",
    " 6) Finally process the file page by page \n",
    "'''\n",
    "#Put your pdf file path \n",
    "\n",
    "base_path = \"C:/Users/Dipti_B/Desktop/ds_keyword_assignment\"\n",
    "\n",
    "\n",
    "my_file = os.path.join(base_path + \"/\" + \"Oreilly.pdf\")\n",
    "log_file = os.path.join(base_path + \"/\" + \"Oreilly.txt\")\n",
    "\n",
    "password = \"\"\n",
    "extracted_text = \"\"\n",
    "\n",
    "# Open and read the pdf file in binary mode\n",
    "fp = open(my_file, \"rb\")\n",
    "\n",
    "# Create parser object to parse the pdf content\n",
    "parser = PDFParser(fp)\n",
    "\n",
    "# Store the parsed content in PDFDocument object\n",
    "document = PDFDocument(parser, password)\n",
    "\n",
    "# Check if document is extractable, if not abort\n",
    "if not document.is_extractable:\n",
    "\traise PDFTextExtractionNotAllowed\n",
    "\t\n",
    "# Create PDFResourceManager object that stores shared resources such as fonts or images\n",
    "rsrcmgr = PDFResourceManager()\n",
    "\n",
    "# set parameters for analysis\n",
    "laparams = LAParams()\n",
    "\n",
    "# Create a PDFDevice object which translates interpreted information into desired format\n",
    "# Device needs to be connected to resource manager to store shared resources\n",
    "# device = PDFDevice(rsrcmgr)\n",
    "# Extract the decive to page aggregator to get LT object elements\n",
    "device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "\n",
    "# Create interpreter object to process page content from PDFDocument\n",
    "# Interpreter needs to be connected to resource manager for shared resources and device \n",
    "interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "# Ok now that we have everything to process a pdf document, lets process it page by page\n",
    "for page in PDFPage.create_pages(document):\n",
    "\t# As the interpreter processes the page stored in PDFDocument object\n",
    "\tinterpreter.process_page(page)\n",
    "\t# The device renders the layout from interpreter\n",
    "\tlayout = device.get_result()\n",
    "\t# Out of the many LT objects within layout, we are interested in LTTextBox and LTTextLine\n",
    "\tfor lt_obj in layout:\n",
    "\t\tif isinstance(lt_obj, LTTextBox) or isinstance(lt_obj, LTTextLine):\n",
    "\t\t\textracted_text += lt_obj.get_text()\n",
    "\t\t\t\n",
    "#close the pdf file\n",
    "fp.close()\n",
    "print(extracted_text);\n",
    "raw=extracted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2:\n",
    "Importing Data & Visualize it\n",
    "\n",
    "this step is importanat to get insight from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can see the raw data\n",
    "#print(raw)\n",
    "print(len(raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3:\n",
    "Preprocessing the data\n",
    "\n",
    "As we want to find keywords from data, first we have to clean it, filter it for further processing\n",
    "\n",
    "Here data is text so we have to remove white spaces, special characters, symbols, stopwords etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we have to find keywords we have to seperate out each word from whole document\n",
    "#this can be done by nltk's tokenize function\n",
    "tokens = word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we have total 5331 tokens\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can see the tokens data\n",
    "#print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation from each word as we have to find keywords punctuation are treated as noise in data\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "#printing first 100 keywords hich are stored in stripped\n",
    "#print(stripped[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking whether the string consists of alphabetic characters only\n",
    "#if yes then only keeping it\n",
    "\n",
    "words=[word for word in stripped if word.isalpha()]\n",
    "#print(words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing length of punctioctions free words\n",
    "#So we are filtering unwanted stuff \n",
    "#print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting all characters to loer case for further processing\n",
    "#This is also called as normelization\n",
    "\n",
    "words_lower=[w.lower() for w in words]\n",
    "#print(words_lower[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stop words\n",
    "# we can see the list of stop words by printing it\n",
    "stop_words = stopwords.words('english')\n",
    "#print(stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering stop words\n",
    "set(stopwords.words('english'))\n",
    "words_stopw_rem = [w for w in words_lower if not w in stop_words]\n",
    "#print(words_stopw_rem[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing length of words after removing stop words\n",
    "#print(len(words_stopw_rem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemitizing is the  process of converting the words of a sentence to its dictionary form. \n",
    "#it is very important as it normalize all words \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words_lemmatized=[lemmatizer.lemmatize(word)for word in words_stopw_rem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(words_lemmatized))\n",
    "#print(words_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(set(words_lemmatized)))\n",
    "sorted((words_lemmatized),reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4:\n",
    "    \n",
    "Getting insight from data\n",
    "\n",
    "All preprocessing task has done now we can play with this data to find the keywords, which is our final goal\n",
    "\n",
    "we can also calculate lexical richness of the text\n",
    "\n",
    "importance or how frequent the specific word has used\n",
    "\n",
    "count of each word in this document\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's calculate a measure of the lexical richness of the text\n",
    "# From this we can say that in document most of the words are repeated as result shows it has 28.9% lexical richness\n",
    "len(set(words_lemmatized))*100 / len(words_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how often a word occurs in a text, and compute what percentage of the text is taken up by a specific word\n",
    "#ex: java\n",
    "100 * words_lemmatized.count('java') / len(words_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_freqDist = nltk.FreqDist(words_lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output top 50 words\n",
    "#It shows how much time that perticular word has repeated in document\n",
    "\n",
    "for word, frequency in words_freqDist.most_common(50):\n",
    "    print(u'{}:{}'.format(word, frequency))\n",
    "   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5:\n",
    "\n",
    "This is the final step\n",
    "\n",
    "After finding the occurrence of each word now we can find the weight of each word\n",
    "\n",
    "Here the document is related to Java language\n",
    "\n",
    "so constraining the word length will remove 'c' which is itself a language\n",
    "\n",
    "so printing the keywords according to their weights and saving the same in CSV format\n",
    "\n",
    "this CSV file is stored in the same folder in which this notebook is saved\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving output in csv file\n",
    "#for this we require pandas package and collection package to deal with freqDist output\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "d=words_freqDist\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(d,orient='index').reset_index()\n",
    "df_new=df\n",
    "#df_new.columns=['index','Keywords']\n",
    "df_new.sort_values(by='index',ascending=True)\n",
    "df_new.columns=['index','Keywords']\n",
    "f=df_new\n",
    "f.sort_values(by='Keywords',ascending=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['Keywords']=round((f['Keywords']*100)/len(words_lemmatized),2)\n",
    "\n",
    "print(f)\n",
    "#Saving it in csv format\n",
    "\n",
    "sorted_df.to_csv(\"keywords_1.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
